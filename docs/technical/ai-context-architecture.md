# AI Counselor Context Architecture

## Overview

The AI counselor (Sesame) requires context to be effective. This document describes how context is assembled, managed, and updated across conversations.

## Context Components

### 1. Profile Narrative
- **What**: Text summary of student's current profile (GPA, testing, activities, schools, etc.)
- **Source**: Generated from `StudentProfile` and related tables
- **Update trigger**: Regenerated when conversation starts
- **Token budget**: ~300 tokens

### 2. Conversation Summary (Long-term Memory)
- **What**: Rolling summary of past conversations - key facts learned, decisions made, open threads, emotional context
- **Source**: Stored in `StudentContext` table, updated after each session
- **Update trigger**: When previous session ends (on new page load, after inactivity)
- **Token budget**: ~200 tokens

### 3. Entry Context
- **What**: Where the user came from (onboarding, planning page, dashboard, etc.) and any initial query
- **Source**: URL params, referrer
- **Update trigger**: Set at conversation start
- **Token budget**: ~50 tokens

### 4. Counselor Objectives
- **What**: Prep sheet for the AI - priority questions, things to check on, updates to share
- **Source**: Generated by background process, stored in DB
- **Update triggers**:
  - After each session ends (prepare for next)
  - Daily cron job (deadline awareness, time-based updates)
  - When significant profile data changes
- **Token budget**: ~100 tokens

### 5. Conversation State (Current Session)
- **What**: What's happened so far this session - topics covered, pending confirmations, emotional tone
- **Source**: Built dynamically each turn from current session messages
- **Update trigger**: Every AI call
- **Token budget**: ~100 tokens

## Session Lifecycle

```
Page Load
    │
    ├── Is there a previous unsummarized session?
    │   └── Yes → Summarize it, update Conversation Summary
    │
    ├── Load all context components
    │
    └── Start new session
            │
            ├── User sends message
            │   ├── Assemble system prompt (all 5 components)
            │   ├── Call AI with messages
            │   ├── Process tool calls → show widgets
            │   └── Update Conversation State
            │
            └── Session ends (inactivity / page unload)
                ├── Summarize session → update Conversation Summary
                └── Regenerate Counselor Objectives
```

## System Prompt Assembly

```
System Prompt = [
  BASE_PERSONA,           // Core personality and instructions
  PROFILE_NARRATIVE,      // Current student profile summary
  CONVERSATION_SUMMARY,   // Past conversations summary
  ENTRY_CONTEXT,          // How they got here
  COUNSELOR_OBJECTIVES,   // What to accomplish
  CONVERSATION_STATE,     // This session so far
  TOOL_INSTRUCTIONS,      // Available tools and how to use them
]
```

## Background Jobs

| Job | Trigger | Purpose |
|-----|---------|---------|
| Session Summarizer | Page load (lazy) or 5min inactivity | Compress completed session into long-term memory |
| Objectives Generator | Session end, daily cron, profile update | Prepare AI for next conversation |

## Token Budget Summary

| Component | Budget |
|-----------|--------|
| Profile Narrative | ~300 |
| Conversation Summary | ~200 |
| Counselor Objectives | ~100 |
| Entry Context | ~50 |
| Conversation State | ~100 |
| **Total Overhead** | **~750 tokens** |

## Dual-Model Architecture (Kimi + Opus)

### Overview

Two models run **in parallel** for each user message:
- **Kimi (Groq)**: Fast (~50ms) - acknowledgment + tool extraction
- **Opus (Claude)**: Deep (~500ms) - substantive response

They are **independent** - Kimi does not brief Opus. Both parse the raw message separately.

### Flow

```
User Message
    │
    ├─────────────────────────────────────┐
    │                                     │
    ▼                                     ▼
KIMI (Fast)                           OPUS (Deep)
    │                                     │
    ├── Parse entities                    ├── Full context (750 tokens)
    ├── Extract tool calls                ├── Deep thinking
    ├── Generate acknowledgment           ├── Empathetic response
    │                                     │
    ▼                                     ▼
Stream ack to user (~50ms)           Stream response (~500ms)
Show widgets immediately             Continues after ack
```

### What Each Model Does

| Model | Context | Job | Output |
|-------|---------|-----|--------|
| Kimi | Slim (~200 tokens) | Parse, acknowledge, extract tools | JSON: `{ tools, ack, entities }` |
| Opus | Full (~750 tokens) | Advise, empathize, think deeply | Natural language response |

### Kimi's Prompt (Slim)

```
You are a fast parser for a college counseling AI.
1. Extract data points (GPA, scores, activities, schools)
2. Decide which tools to call
3. Generate a brief acknowledgment (1 sentence max)

Do NOT give advice. Do NOT answer questions. Just parse and acknowledge.

Student: [name], [grade]
Entry: [where they came from]
```

### Opus's Prompt (Full)

Uses all 5 context components documented above:
- Profile Narrative
- Conversation Summary
- Entry Context
- Counselor Objectives
- Conversation State

### Why Parallel, Not Sequential?

- **Latency**: User sees response start in 50ms, not 550ms
- **Independence**: Opus is smart enough to parse on its own
- **Simplicity**: No coordination needed between models

The only "handoff" is at the stream level - Kimi's output appears first, Opus continues.

### Token Budgets by Model

| Model | Context Budget | Purpose |
|-------|---------------|---------|
| Kimi | ~200 tokens | Just enough to parse |
| Opus | ~750 tokens | Full context for deep thinking |

## Key Design Decisions

1. **Lazy summarization**: Previous session is summarized when new session starts, not immediately after session ends
2. **Hybrid state management**: React state for UI (widgets), system prompt for AI context
3. **Proactive objectives**: AI comes prepared with goals, not just reactive to user
4. **Narrative over JSON**: Profile context is human-readable text for the AI, not raw JSON
5. **Parallel models**: Kimi and Opus run independently for speed, no briefing between them
